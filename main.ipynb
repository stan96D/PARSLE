{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stan-\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import stanza\n",
    "import csv\n",
    "\n",
    "import os\n",
    "os.environ['R_HOME'] = 'C:\\Program Files\\R\\R-4.3.1'\n",
    "\n",
    "from asyncio.windows_events import NULL\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import plotly.express as px\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "\n",
    "class Feedback:\n",
    "\n",
    "    def __init__(self, text, name, role, nameFor, assessment, scale):\n",
    "        self.text = text\n",
    "        self.name = name\n",
    "        self.nameFor = nameFor\n",
    "        self.role = role\n",
    "        self.assessment = assessment\n",
    "        self.scale = scale\n",
    "        self.feedback = None\n",
    "        self.feedup = None\n",
    "        self.feedforward = None\n",
    "\n",
    "    def setFeedup(self, value):\n",
    "        self.feedup = value\n",
    "\n",
    "    def setFeedback(self, value):\n",
    "        self.feedback = value\n",
    "\n",
    "    def setFeedforward(self, value):\n",
    "        self.feedforward = value\n",
    "\n",
    "\n",
    "def createModel(trainingSet, amount):\n",
    "    # Read the Excel file, skipping the first row with column names\n",
    "    df = pd.read_excel(trainingSet + '.xlsx', skiprows=1)\n",
    "\n",
    "    nlp = stanza.Pipeline(processors='tokenize,pos,lemma', lang='en')\n",
    "\n",
    "    # Define sentences and labels\n",
    "    sentences = df.iloc[0:int(amount), 0].tolist()\n",
    "    labels = df.iloc[0:int(amount), 1].tolist()\n",
    "\n",
    "    # Preprocess the sentences using Stanza\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        preprocessed_sentence = ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "\n",
    "    # Set device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    sentences_train, sentences_val, labels_train, labels_val = train_test_split(\n",
    "        preprocessed_sentences, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Load pre-trained BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Tokenize and encode the sentences\n",
    "    inputs_train = tokenizer(sentences_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs_val = tokenizer(sentences_val, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Encode the labels\n",
    "    label_mapping = {label: i for i, label in enumerate(set(labels))}\n",
    "    labels_train_encoded = [label_mapping[label] for label in labels_train]\n",
    "    labels_val_encoded = [label_mapping[label] for label in labels_val]\n",
    "\n",
    "    # Create dataset and dataloaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(inputs_train[\"input_ids\"], inputs_train[\"attention_mask\"], torch.tensor(labels_train_encoded))\n",
    "    val_dataset = torch.utils.data.TensorDataset(inputs_val[\"input_ids\"], inputs_val[\"attention_mask\"], torch.tensor(labels_val_encoded))\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    # Load pre-trained BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_mapping))\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"Model created\")\n",
    "\n",
    "    return train_dataloader, val_dataloader, model, device\n",
    "\n",
    "\n",
    "def trainModel(model, device, train_dataloader, val_dataloader):\n",
    "\n",
    "    # Set optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 1\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted_labels = logits.max(1)\n",
    "                num_correct += (predicted_labels == labels).sum().item()\n",
    "                num_samples += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracy = num_correct / num_samples * 100\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    model.save_pretrained(\"saved_model/model\")\n",
    "    print(\"Model saved in \" + \"saved_model/model\")\n",
    "    return \n",
    "\n",
    "def extractExcelData():\n",
    "\n",
    "    # Extract the Excel data\n",
    "    df = pd.read_excel('data_set.xlsx', skiprows=1)\n",
    "\n",
    "    index = 0\n",
    "    feedback_data = []\n",
    "    input_sentences = []\n",
    "\n",
    "    while index < df.shape[0]:\n",
    "\n",
    "        feedback = Feedback(df.iloc[index].values[0],\n",
    "                            df.iloc[index].values[1],\n",
    "                            df.iloc[index].values[2],\n",
    "                            df.iloc[index].values[3],\n",
    "                            df.iloc[index].values[4],\n",
    "                            df.iloc[index].values[5])\n",
    "        \n",
    "        feedback_data.append(feedback)\n",
    "        input_sentences.append(df.iloc[index].values[0])\n",
    "        index += 1 \n",
    "\n",
    "    return feedback_data, input_sentences\n",
    "\n",
    "def evaluateModel(input_sentences, loaded_model):\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # For multiple sentences\n",
    "    input_ids = tokenizer.batch_encode_plus(input_sentences, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move input tensors to the device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    loaded_model.eval()\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_model(input_ids)[0]  # Access the logits from the model output\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        print(probabilities)\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "        print(predicted_labels)\n",
    "\n",
    "    # Convert predicted labels to class names\n",
    "    label_mapping = {0: \"feedback\", 1: \"feedforward\", 2: \"feedup\"}\n",
    "    predicted_labels = [label_mapping[label.item()] for label in predicted_labels]\n",
    "    print(label_mapping, predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "def exportCSV(predicted_labels, feedback_data):\n",
    "\n",
    "    index = 0\n",
    "    dataCSV = []\n",
    "    while index < len(feedback_data):\n",
    "\n",
    "        fb = feedback_data[index]\n",
    "        fb.setFeedback(int(predicted_labels[index] == 'feedback'))\n",
    "        fb.setFeedup(int(predicted_labels[index] == 'feedup'))\n",
    "        fb.setFeedforward(int(predicted_labels[index] == 'feedforward'))\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    dataCSV = [\n",
    "        ['user', 'role', 'scale', 'assessment', 'for', 'text', 'feedback', 'feedup', 'feedforward'],\n",
    "    ]\n",
    "\n",
    "    for fb in feedback_data:\n",
    "\n",
    "        dataCSV.append([fb.name, fb.role, fb.scale, fb.assessment, fb.nameFor, fb.text, fb.feedback, fb.feedup, fb.feedforward])\n",
    "\n",
    "    # Specify the file path and name for the CSV file\n",
    "    csv_file = 'data_ENA.csv'\n",
    "\n",
    "    # Open the file in write mode and create a CSV writer object\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerows(dataCSV)\n",
    "\n",
    "    print(\"CSV file created successfully in data_ENA.csv.\")\n",
    "\n",
    "    return\n",
    "\n",
    "def createDataFrame():\n",
    "    df = pd.read_csv(\"./data_ENA.csv\")\n",
    "\n",
    "    # df\n",
    "    with (robjects.default_converter + pandas2ri.converter).context():\n",
    "        r_from_pd_df = robjects.conversion.get_conversion().py2rpy(df)\n",
    "\n",
    "    return r_from_pd_df\n",
    "\n",
    "def createENA(r_from_pd_df):\n",
    "\n",
    "    base = importr('base')\n",
    "    rena = importr('rENA')\n",
    "\n",
    "    units = robjects.StrVector(['Role', 'UserName'])\n",
    "    convs = robjects.StrVector(['Role', 'Assessment'])\n",
    "    codes = robjects.StrVector(['Feedback', 'Feedup', 'Feedforward'])\n",
    "    metadata = robjects.StrVector(['Scale', 'Text', 'For'])\n",
    "\n",
    "    unit_cols_i = robjects.IntVector((2,1))\n",
    "    units_sub_df = r_from_pd_df.rx(True, unit_cols_i)\n",
    "    convs_cols_i = robjects.IntVector((2,4))\n",
    "    convs_sub_df = r_from_pd_df.rx(True, convs_cols_i)\n",
    "    codes_cols_i = robjects.IntVector((7,8,9))\n",
    "    codes_sub_df = r_from_pd_df.rx(True, codes_cols_i)\n",
    "    metadata_cols_i = robjects.IntVector((3,6,5))\n",
    "    metadata_sub_df = r_from_pd_df.rx(True, metadata_cols_i)\n",
    "\n",
    "    ena_accum = robjects.r('ena.accumulate.data')\n",
    "    accum = ena_accum(units_sub_df, convs_sub_df, codes_sub_df, metadata = metadata_sub_df)\n",
    "\n",
    "    ena_set = robjects.r('ena.make.set')\n",
    "    model = ena_set(accum)\n",
    "\n",
    "    ena_plot = robjects.r('ena.plot')\n",
    "    ena_plot_points = robjects.r('ena.plot.points')\n",
    "\n",
    "    model_points = model.rx('points')[0]\n",
    "    model_points_mtx = robjects.r('as.matrix')(model_points)\n",
    "    model_plot = ena_plot(model)\n",
    "    model_plot = ena_plot_points(model_plot, model_points_mtx)\n",
    "\n",
    "    return model_plot\n",
    "\n",
    "\n",
    "def visualizeENA(model_plot):\n",
    "    html_print = robjects.r('htmltools::html_print')\n",
    "    as_tags = robjects.r('htmltools::as.tags')\n",
    "    model_html = html_print(as_tags(model_plot['plot'], standalone = True), viewer = robjects.r(\"NULL\"))[0]\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(model_html)\n",
    "    import webbrowser\n",
    "    webbrowser.open_new_tab(model_html)\n",
    "\n",
    "def main():\n",
    "    trainModelAnswer = input(\"Do you want to create a new model? It will override the current one. (Y/N).\")\n",
    "    if (trainModelAnswer == \"Y\" or trainModelAnswer == \"y\"):\n",
    "\n",
    "        createModelSetName = input(\"What is the name of the dataset?\")\n",
    "        createModelRowCount = input(\"What is the amount of data in rows?\")\n",
    "\n",
    "        print(\"Preprocessing data....\")\n",
    "        train_dataloader, val_dataloader, model, device = createModel(createModelSetName, createModelRowCount)\n",
    "\n",
    "        print(\"Training started....\")\n",
    "        trainModel(model, device, train_dataloader, val_dataloader)\n",
    "\n",
    "\n",
    "    loadModelAnswer = input(\"Do you want to test the model? It will generate CSV data for the ENA. (Y/N).\")\n",
    "    if (loadModelAnswer == \"Y\" or loadModelAnswer == \"y\"):\n",
    "\n",
    "        # Load the Model\n",
    "        loaded_model = BertForSequenceClassification.from_pretrained(\"saved_model/model\")\n",
    "\n",
    "        feedback_data, input_sentences = extractExcelData()\n",
    "        predicted_labels = evaluateModel(input_sentences, loaded_model)\n",
    "        exportCSV(predicted_labels, feedback_data)\n",
    "\n",
    "    showENAAnswer = input(\"Do you want to plot the CSV in an ENA? It will open a html web browser. (Y/N).\")\n",
    "    if (showENAAnswer == \"Y\" or showENAAnswer == \"y\"):\n",
    "        df = createDataFrame()\n",
    "        model = createENA(df)\n",
    "        visualizeENA(model)\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
